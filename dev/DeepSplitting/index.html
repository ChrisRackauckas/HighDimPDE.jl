<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>The DeepSplitting algorithm ¬∑ HighDimPDE.jl</title><meta name="title" content="The DeepSplitting algorithm ¬∑ HighDimPDE.jl"/><meta property="og:title" content="The DeepSplitting algorithm ¬∑ HighDimPDE.jl"/><meta property="twitter:title" content="The DeepSplitting algorithm ¬∑ HighDimPDE.jl"/><meta name="description" content="Documentation for HighDimPDE.jl."/><meta property="og:description" content="Documentation for HighDimPDE.jl."/><meta property="twitter:description" content="Documentation for HighDimPDE.jl."/><meta property="og:url" content="https://docs.sciml.ai/HighDimPDE/stable/DeepSplitting/"/><meta property="twitter:url" content="https://docs.sciml.ai/HighDimPDE/stable/DeepSplitting/"/><link rel="canonical" href="https://docs.sciml.ai/HighDimPDE/stable/DeepSplitting/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="HighDimPDE.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">HighDimPDE.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting started</a></li><li><a class="tocitem" href="../problems/">Problems</a></li><li><span class="tocitem">Solver Algorithms</span><ul><li><a class="tocitem" href="../MLP/">The <code>MLP</code> algorithm</a></li><li class="is-active"><a class="tocitem" href>The <code>DeepSplitting</code> algorithm</a><ul class="internal"><li><a class="tocitem" href="#The-general-idea"><span>The general idea üí°</span></a></li><li><a class="tocitem" href="#Solving-point-wise-or-on-a-hypercube"><span>Solving point-wise or on a hypercube</span></a></li><li><a class="tocitem" href="#Non-local-PDEs"><span>Non-local PDEs</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../DeepBSDE/">The <code>DeepBSDE</code> algorithm</a></li><li><a class="tocitem" href="../NNStopping/">The <code>NNStopping</code> algorithm</a></li><li><a class="tocitem" href="../NNKolmogorov/">The <code>NNKolmogorov</code> algorithm</a></li><li><a class="tocitem" href="../NNParamKolmogorov/">The <code>NNParamKolmogorov</code> algorithm</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorials/deepsplitting/"><code>DeepSplitting</code></a></li><li><a class="tocitem" href="../tutorials/deepbsde/"><code>DeepBSDE</code></a></li><li><a class="tocitem" href="../tutorials/mlp/"><code>MLP</code></a></li><li><a class="tocitem" href="../tutorials/nnstopping/"><code>NNStopping</code></a></li><li><a class="tocitem" href="../tutorials/nnkolmogorov/"><code>NNKolmogorov</code></a></li><li><a class="tocitem" href="../tutorials/nnparamkolmogorov/"><code>NNParamKolmogorov</code></a></li></ul></li><li><a class="tocitem" href="../Feynman_Kac/">Feynman Kac formula</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Solver Algorithms</a></li><li class="is-active"><a href>The <code>DeepSplitting</code> algorithm</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>The <code>DeepSplitting</code> algorithm</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/HighDimPDE.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/HighDimPDE.jl/blob/main/docs/src/DeepSplitting.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="deepsplitting"><a class="docs-heading-anchor" href="#deepsplitting">The <code>DeepSplitting</code> algorithm</a><a id="deepsplitting-1"></a><a class="docs-heading-anchor-permalink" href="#deepsplitting" title="Permalink"></a></h1><h3 id="Problems-Supported:"><a class="docs-heading-anchor" href="#Problems-Supported:">Problems Supported:</a><a id="Problems-Supported:-1"></a><a class="docs-heading-anchor-permalink" href="#Problems-Supported:" title="Permalink"></a></h3><ol><li><a href="../problems/#HighDimPDE.PIDEProblem"><code>PIDEProblem</code></a></li><li><a href="../problems/#HighDimPDE.ParabolicPDEProblem"><code>ParabolicPDEProblem</code></a></li></ol><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="HighDimPDE.DeepSplitting" href="#HighDimPDE.DeepSplitting"><code>HighDimPDE.DeepSplitting</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DeepSplitting(nn, K=1, opt = Flux.Optimise.Adam(0.01), Œªs = nothing, mc_sample =  NoSampling())</code></pre><p>Deep splitting algorithm.</p><p><strong>Arguments</strong></p><ul><li><code>nn</code>: a <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.Chain">Flux.Chain</a>, or more generally a <a href="https://github.com/FluxML/Functors.jl">functor</a>.</li><li><code>K</code>: the number of Monte Carlo integrations.</li><li><code>opt</code>: optimizer to be used. By default, <code>Flux.Optimise.Adam(0.01)</code>.</li><li><code>Œªs</code>: the learning rates, used sequentially. Defaults to a single value taken from <code>opt</code>.</li><li><code>mc_sample::MCSampling</code> : sampling method for Monte Carlo integrations of the non-local term. Can be <code>UniformSampling(a,b)</code>, <code>NormalSampling(œÉ_sampling, shifted)</code>, or <code>NoSampling</code> (by default).</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">hls = d + 50 # hidden layer size
d = 10 # size of the sample

# Neural network used by the scheme
nn = Flux.Chain(Dense(d, hls, tanh),
                Dense(hls,hls,tanh),
                Dense(hls, 1, x-&gt;x^2))

alg = DeepSplitting(nn, K=10, opt = Flux.Optimise.Adam(), Œªs = [5e-3,1e-3],
                    mc_sample = UniformSampling(zeros(d), ones(d)) )</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/HighDimPDE.jl/blob/d9fd208c79bd49728e5eb10a93fa2d539acfcc74/src/DeepSplitting.jl#L7-L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="CommonSolve.solve-Tuple{Union{PIDEProblem, ParabolicPDEProblem}, DeepSplitting, Any}" href="#CommonSolve.solve-Tuple{Union{PIDEProblem, ParabolicPDEProblem}, DeepSplitting, Any}"><code>CommonSolve.solve</code></a> ‚Äî <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solve(
    prob::Union{PIDEProblem, ParabolicPDEProblem},
    alg::DeepSplitting,
    dt;
    batch_size,
    abstol,
    verbose,
    maxiters,
    use_cuda,
    cuda_device,
    verbose_rate
) -&gt; PIDESolution{_A, _B, _C, Vector{_A1}, Vector{Any}, Nothing} where {_A, _B, _C, _A1}
</code></pre><p>Returns a <code>PIDESolution</code> object.</p><p><strong>Arguments</strong></p><ul><li><code>maxiters</code>: number of iterations per time step. Can be a tuple, where <code>maxiters[1]</code> is used for the training of the neural network used in the first time step (which can be long) and <code>maxiters[2]</code> is used for the rest of the time steps.</li><li><code>batch_size</code> : the batch size.</li><li><code>abstol</code> : threshold for the objective function under which the training is stopped.</li><li><code>verbose</code> : print training information.</li><li><code>verbose_rate</code> : rate for printing training information (every <code>verbose_rate</code> iterations).</li><li><code>use_cuda</code> : set to <code>true</code> to use CUDA.</li><li><code>cuda_device</code> : integer, to set the CUDA device used in the training, if <code>use_cuda == true</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/HighDimPDE.jl/blob/d9fd208c79bd49728e5eb10a93fa2d539acfcc74/src/DeepSplitting.jl#L53">source</a></section></article><p>The <code>DeepSplitting</code> algorithm reformulates the PDE as a stochastic learning problem.</p><p>The algorithm relies on two main ideas:</p><ul><li><p>The approximation of the solution <span>$u$</span> by a parametric function <span>$\bf u^\theta$</span>. This function is generally chosen as a (Feedforward) Neural Network, as it is a <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal approximator</a>.</p></li><li><p>The training of <span>$\bf u^\theta$</span> by simulated stochastic trajectories of particles, through the link between linear PDEs and the expected trajectory of associated Stochastic Differential Equations (SDEs), explicitly stated by the <a href="https://en.wikipedia.org/wiki/Feynman‚ÄìKac_formula">Feynman Kac formula</a>.</p></li></ul><h2 id="The-general-idea"><a class="docs-heading-anchor" href="#The-general-idea">The general idea üí°</a><a id="The-general-idea-1"></a><a class="docs-heading-anchor-permalink" href="#The-general-idea" title="Permalink"></a></h2><p>Consider the PDE</p><p class="math-container">\[\partial_t u(t,x) = \mu(t, x) \nabla_x u(t,x) + \frac{1}{2} \sigma^2(t, x) \Delta_x u(t,x) + f(x, u(t,x)) \tag{1}\]</p><p>with initial conditions <span>$u(0, x) = g(x)$</span>, where <span>$u \colon \R^d \to \R$</span>. </p><h3 id="Local-Feynman-Kac-formula"><a class="docs-heading-anchor" href="#Local-Feynman-Kac-formula">Local Feynman Kac formula</a><a id="Local-Feynman-Kac-formula-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Feynman-Kac-formula" title="Permalink"></a></h3><p><code>DeepSplitting</code> solves the PDE iteratively over small time intervals by using an approximate <a href="../Feynman_Kac/#feynmankac">Feynman-Kac representation</a> locally.</p><p>More specifically, considering a small time step <span>$dt = t_{n+1} - t_n$</span> one has that</p><p class="math-container">\[u(t_{n+1}, X_{T - t_{n+1}}) \approx \mathbb{E} \left[ f(t, X_{T - t_{n}}, u(t_{n},X_{T - t_{n}}))(t_{n+1} - t_n) + u(t_{n}, X_{T - t_{n}}) | X_{T - t_{n+1}}\right] \tag{3}.\]</p><p>One can therefore use Monte Carlo integrations to approximate the expectations</p><p class="math-container">\[u(t_{n+1}, X_{T - t_{n+1}}) \approx \frac{1}{\text{batch\_size}}\sum_{j=1}^{\text{batch\_size}} \left[ u(t_{n}, X_{T - t_{n}}^{(j)}) + (t_{n+1} - t_n)\sum_{k=1}^{K} \big[ f(t_n, X_{T - t_{n}}^{(j)}, u(t_{n},X_{T - t_{n}}^{(j)})) \big] \right]\]</p><h3 id="Reformulation-as-a-learning-problem"><a class="docs-heading-anchor" href="#Reformulation-as-a-learning-problem">Reformulation as a learning problem</a><a id="Reformulation-as-a-learning-problem-1"></a><a class="docs-heading-anchor-permalink" href="#Reformulation-as-a-learning-problem" title="Permalink"></a></h3><p>The <code>DeepSplitting</code> algorithm approximates <span>$u(t_{n+1}, x)$</span> by a parametric function <span>${\bf u}^\theta_n(x)$</span>. It is advised to let this function be a neural network <span>${\bf u}_\theta \equiv NN_\theta$</span> as they are universal approximators.</p><p>For each time step <span>$t_n$</span>, the <code>DeepSplitting</code> algorithm </p><ol><li><p>Generates the particle trajectories <span>$X^{x, (j)}$</span> satisfying <a href="../Feynman_Kac/#feynmankac">Eq. (2)</a> over the whole interval <span>$[0,T]$</span>.</p></li><li><p>Seeks <span>${\bf u}_{n+1}^{\theta}$</span>  by minimizing the loss function</p></li></ol><p class="math-container">\[L(\theta) = ||{\bf u}^\theta_{n+1}(X_{T - t_n}) - \left[ f(t, X_{T - t_{n-1}}, {\bf u}_{n-1}(X_{T - t_{n-1}}))(t_{n} - t_{n-1}) + {\bf u}_{n-1}(X_{T - t_{n-1}}) \right] ||\]</p><p>This way, the PDE approximation problem is decomposed into a sequence of separate learning problems. In <code>HighDimPDE.jl</code> the right parameter combination <span>$\theta$</span> is found by iteratively minimizing <span>$L$</span> using <strong>stochastic gradient descent</strong>.</p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>To solve with <code>DeepSplitting</code>, one needs to provide to <code>solve</code></p><ul><li><code>dt</code></li><li><code>batch_size</code></li><li><code>maxiters</code>: the number of iterations for minimizing the loss function</li><li><code>abstol</code>: the absolute tolerance for the loss function</li><li><code>use_cuda</code>: if you have a Nvidia GPU, recommended.</li></ul></div></div><h2 id="Solving-point-wise-or-on-a-hypercube"><a class="docs-heading-anchor" href="#Solving-point-wise-or-on-a-hypercube">Solving point-wise or on a hypercube</a><a id="Solving-point-wise-or-on-a-hypercube-1"></a><a class="docs-heading-anchor-permalink" href="#Solving-point-wise-or-on-a-hypercube" title="Permalink"></a></h2><h3 id="Pointwise"><a class="docs-heading-anchor" href="#Pointwise">Pointwise</a><a id="Pointwise-1"></a><a class="docs-heading-anchor-permalink" href="#Pointwise" title="Permalink"></a></h3><p><code>DeepSplitting</code> allows obtaining <span>$u(t,x)$</span> on a single point  <span>$x \in \Omega$</span> with the keyword <span>$x$</span>.</p><pre><code class="language-julia hljs">prob = PIDEProblem(Œº, œÉ, x, tspan, g, f,)</code></pre><h3 id="Hypercube"><a class="docs-heading-anchor" href="#Hypercube">Hypercube</a><a id="Hypercube-1"></a><a class="docs-heading-anchor-permalink" href="#Hypercube" title="Permalink"></a></h3><p>Yet more generally, one wants to solve Eq. (1) on a <span>$d$</span>-dimensional cube <span>$[a,b]^d$</span>. This is offered by <code>HighDimPDE.jl</code> with the keyword <code>x0_sample</code>.</p><pre><code class="language-julia hljs">prob = PIDEProblem(Œº, œÉ, x, tspan, g, f; x0_sample = x0_sample)</code></pre><p>Internally, this is handled by assigning a random variable as the initial point of the particles, i.e.</p><p class="math-container">\[X_t^\xi = \int_0^t \mu(X_s^x)ds + \int_0^t\sigma(X_s^x)dB_s + \xi,\]</p><p>where <span>$\xi$</span> a random variable uniformly distributed over <span>$[a,b]^d$</span>. This way, the neural network is trained on the whole interval <span>$[a,b]^d$</span> instead of a single point.</p><h2 id="Non-local-PDEs"><a class="docs-heading-anchor" href="#Non-local-PDEs">Non-local PDEs</a><a id="Non-local-PDEs-1"></a><a class="docs-heading-anchor-permalink" href="#Non-local-PDEs" title="Permalink"></a></h2><p><code>DeepSplitting</code> can solve for non-local reaction diffusion equations of the type</p><p class="math-container">\[\partial_t u = \mu(x) \nabla_x u + \frac{1}{2} \sigma^2(x) \Delta u + \int_{\Omega}f(x,y, u(t,x), u(t,y))dy\]</p><p>The non-localness is handled by a Monte Carlo integration.</p><p class="math-container">\[u(t_{n+1}, X_{T - t_{n+1}}) \approx \sum_{j=1}^{\text{batch\_size}} \left[ u(t_{n}, X_{T - t_{n}}^{(j)}) + \frac{(t_{n+1} - t_n)}{K}\sum_{k=1}^{K} \big[ f(t, X_{T - t_{n}}^{(j)}, Y_{X_{T - t_{n}}^{(j)}}^{(k)}, u(t_{n},X_{T - t_{n}}^{(j)}), u(t_{n},Y_{X_{T - t_{n}}^{(j)}}^{(k)})) \big] \right]\]</p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>In practice, if you have a non-local model, you need to provide the sampling method and the number <span>$K$</span> of MC integration through the keywords <code>mc_sample</code> and <code>K</code>. </p><pre><code class="language-julia hljs">alg = DeepSplitting(nn, opt = opt, mc_sample = mc_sample, K = 1)</code></pre><p><code>mc_sample</code> can be whether <code>UniformSampling(a, b)</code> or <code>NormalSampling(œÉ_sampling, shifted)</code>.</p></div></div><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><ul><li>Boussange, V., Becker, S., Jentzen, A., Kuckuck, B., Pellissier, L., Deep learning approximations for non-local nonlinear PDEs with Neumann boundary conditions. <a href="https://arxiv.org/abs/2205.03672">arXiv</a> (2022)</li><li>Beck, C., Becker, S., Cheridito, P., Jentzen, A., Neufeld, A., Deep splitting method for parabolic PDEs. <a href="https://arxiv.org/abs/1907.03452">arXiv</a> (2019)</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../MLP/">¬´ The <code>MLP</code> algorithm</a><a class="docs-footer-nextpage" href="../DeepBSDE/">The <code>DeepBSDE</code> algorithm ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Friday 31 May 2024 01:24">Friday 31 May 2024</span>. Using Julia version 1.10.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
