var documenterSearchIndex = {"docs":
[{"location":"examples/blackscholes/#Solving-the-100-dimensional-Black-Scholes-Barenblatt-Equation","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"","category":"section"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"Black Scholes equation is a model for stock option price. In 1973, Black and Scholes transformed their formula on option pricing and corporate liabilities into a PDE model, which is widely used in financing engineering for computing the option price over time. [1.] In this example, we will solve a Black-Scholes-Barenblatt equation of 100 dimensions. The Black-Scholes-Barenblatt equation is a nonlinear extension to the Black-Scholes equation, which models uncertain volatility and interest rates derived from the Black-Scholes equation. This model results in a nonlinear PDE whose dimension is the number of assets in the portfolio.","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"To solve it using the PIDEProblem, we write:","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"d = 100 # number of dimensions\nX0 = repeat([1.0f0, 0.5f0], div(d, 2)) # initial value of stochastic state\ntspan = (0.0f0, 1.0f0)\nr = 0.05f0\nsigma = 0.4f0\nf(X, u, σᵀ∇u, p, t) = r * (u - sum(X .* σᵀ∇u))\ng(X) = sum(X .^ 2)\nμ_f(X, p, t) = zero(X) #Vector d x 1\nσ_f(X, p, t) = Diagonal(sigma * X) #Matrix d x d\nprob = PIDEProblem(g, f, μ_f, σ_f, X0, tspan)","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"As described in the API docs, we now need to define our NNPDENS algorithm by giving it the Flux.jl chains we want it to use for the neural networks. u0 needs to be a d-dimensional -> 1-dimensional chain, while σᵀ∇u needs to be d+1-dimensional to d dimensions. Thus we define the following:","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"hls = 10 + d #hide layer size\nopt = Flux.Optimise.Adam(0.001)\nu0 = Flux.Chain(Dense(d, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, 1))\nσᵀ∇u = Flux.Chain(Dense(d + 1, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, d))\npdealg = NNPDENS(u0, σᵀ∇u, opt = opt)","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"And now we solve the PDE. Here, we say we want to solve the underlying neural SDE using the Euler-Maruyama SDE solver with our chosen dt=0.2, do at most 150 iterations of the optimizer, 100 SDE solves per loss evaluation (for averaging), and stop if the loss ever goes below 1f-6.","category":"page"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"ans = solve(prob, pdealg, verbose = true, maxiters = 150, trajectories = 100,\n    alg = EM(), dt = 0.2, pabstol = 1.0f-6)","category":"page"},{"location":"examples/blackscholes/#References","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"References","text":"","category":"section"},{"location":"examples/blackscholes/","page":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"Shinde, A. S., and K. C. Takale. \"Study of Black-Scholes model and its applications.\" Procedia Engineering 38 (2012): 270-279.","category":"page"},{"location":"tutorials/nnstopping/#NNStopping","page":"NNStopping","title":"NNStopping","text":"","category":"section"},{"location":"tutorials/nnstopping/#Solving-for-optimal-strategy-and-expected-payoff-of-a-Bermudan-Max-Call-option","page":"NNStopping","title":"Solving for optimal strategy and expected payoff of a Bermudan Max-Call option","text":"","category":"section"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"We will calculate optimal strategy for Bermudan Max-Call option with following drift, diffusion and payoff:","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"μ(x) =(r  δ) x σ(x) = β diag(x1   xd)\ng(t x) =  e^-rtmaxlbrace maxlbrace x1   xd rbrace  K 0rbrace","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"We define the parameters, drift function and the diffusion function for the dynamics of the option.","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"using HighDimPDE, Flux, StochasticDiffEq\nd = 3 # Number of assets in the stock\nr = 0.05 # interest rate\nbeta = 0.2 # volatility\nT = 3.0 # maturity\nu0 = fill(90.0, d) # initial stock value\ndelta = 0.1 # delta\nf(du, u, p, t) = du .= (r - delta) * u # drift\nsigma(du, u, p, t) = du .= beta * u # diffusion\ntspan = (0.0, T)\nN = 9 # discretization parameter\ndt = T / (N)\nK = 100.00 # strike price\n\n# payoff function\nfunction g(x, t)\n    return exp(-r * t) * (max(maximum(x) - K, 0))\nend","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"We then define a ParabolicPDEProblem with no non linear term:","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"prob = ParabolicPDEProblem(f, sigma, u0, tspan; payoff = g)","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"note: Note\n","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"We provide the payoff function with a keyword argument payoff","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"And now we define our models:","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"models = [Chain(Dense(d + 1, 32, tanh), BatchNorm(32, tanh), Dense(32, 1, sigmoid))\n          for i in 1:N]","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"note: Note\n","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"The number of models should be equal to the time discritization.","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"And finally we define our optimizer and algorithm, and call solve:","category":"page"},{"location":"tutorials/nnstopping/","page":"NNStopping","title":"NNStopping","text":"opt = Flux.Optimisers.Adam(0.01)\nalg = NNStopping(models, opt)\n\nsol = solve(\n    prob, alg, SRIW1(); dt = dt, trajectories = 1000, maxiters = 1000, verbose = true)","category":"page"},{"location":"tutorials/mlp/#Solving-the-10-dimensional-Fisher-KPP-equation-with-MLP","page":"Solving the 10-dimensional Fisher-KPP equation with MLP","title":"Solving the 10-dimensional Fisher-KPP equation with MLP","text":"","category":"section"},{"location":"tutorials/mlp/","page":"Solving the 10-dimensional Fisher-KPP equation with MLP","title":"Solving the 10-dimensional Fisher-KPP equation with MLP","text":"Let's solve the Fisher KPP PDE in dimension 10 with MLP.","category":"page"},{"location":"tutorials/mlp/","page":"Solving the 10-dimensional Fisher-KPP equation with MLP","title":"Solving the 10-dimensional Fisher-KPP equation with MLP","text":"partial_t u = u (1 - u) + frac12sigma^2Delta_xu tag1","category":"page"},{"location":"tutorials/mlp/","page":"Solving the 10-dimensional Fisher-KPP equation with MLP","title":"Solving the 10-dimensional Fisher-KPP equation with MLP","text":"using HighDimPDE\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0, 0.5) # time horizon\nx0 = fill(0.0, d)  # initial point\ng(x) = exp(-sum(x .^ 2)) # initial condition\nμ(x, p, t) = 0.0 # advection coefficients\nσ(x, p, t) = 0.1 # diffusion coefficients\nf(x, v_x, ∇v_x, p, t) = max(0.0, v_x) * (1 - max(0.0, v_x)) # nonlocal nonlinear part of the\nprob = ParabolicPDEProblem(μ, σ, x0, tspan; g, f) # defining the problem\n\n## Definition of the algorithm\nalg = MLP() # defining the algorithm. We use the Multi Level Picard algorithm\n\n## Solving with multiple threads \nsol = solve(prob, alg, multithreading = true)","category":"page"},{"location":"tutorials/mlp/#Non-local-PDE-with-Neumann-boundary-conditions","page":"Solving the 10-dimensional Fisher-KPP equation with MLP","title":"Non local PDE with Neumann boundary conditions","text":"","category":"section"},{"location":"tutorials/mlp/","page":"Solving the 10-dimensional Fisher-KPP equation with MLP","title":"Solving the 10-dimensional Fisher-KPP equation with MLP","text":"Let's include in the previous equation non local competition, i.e.","category":"page"},{"location":"tutorials/mlp/","page":"Solving the 10-dimensional Fisher-KPP equation with MLP","title":"Solving the 10-dimensional Fisher-KPP equation with MLP","text":"partial_t u = u (1 - int_Omega u(ty)dy) + frac12sigma^2Delta_xu tag2","category":"page"},{"location":"tutorials/mlp/","page":"Solving the 10-dimensional Fisher-KPP equation with MLP","title":"Solving the 10-dimensional Fisher-KPP equation with MLP","text":"where Omega = -12 12^d, and let's assume Neumann Boundary condition on Omega.","category":"page"},{"location":"tutorials/mlp/","page":"Solving the 10-dimensional Fisher-KPP equation with MLP","title":"Solving the 10-dimensional Fisher-KPP equation with MLP","text":"using HighDimPDE\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0, 0.5) # time horizon\nx0 = fill(0.0, d)  # initial point\ng(x) = exp(-sum(x .^ 2)) # initial condition\nμ(x, p, t) = 0.0 # advection coefficients\nσ(x, p, t) = 0.1 # diffusion coefficients\nmc_sample = UniformSampling(fill(-5.0f-1, d), fill(5.0f-1, d))\nf(x, y, v_x, v_y, ∇v_x, ∇v_y, p, t) = max(0.0, v_x) * (1 - max(0.0, v_y))\nprob = PIDEProblem(μ, σ, x0, tspan, g, f) # defining x0_sample is sufficient to implement Neumann boundary conditions\n\n## Definition of the algorithm\nalg = MLP(mc_sample = mc_sample)\n\nsol = solve(prob, alg, multithreading = true)","category":"page"},{"location":"DeepBSDE/#deepbsde","page":"The DeepBSDE algorithm","title":"The DeepBSDE algorithm","text":"","category":"section"},{"location":"DeepBSDE/#Problems-Supported:","page":"The DeepBSDE algorithm","title":"Problems Supported:","text":"","category":"section"},{"location":"DeepBSDE/","page":"The DeepBSDE algorithm","title":"The DeepBSDE algorithm","text":"ParabolicPDEProblem","category":"page"},{"location":"DeepBSDE/","page":"The DeepBSDE algorithm","title":"The DeepBSDE algorithm","text":"Modules = [HighDimPDE]\nPages   = [\"DeepBSDE.jl\", \"DeepBSDE_Han.jl\"]","category":"page"},{"location":"DeepBSDE/#HighDimPDE.DeepBSDE","page":"The DeepBSDE algorithm","title":"HighDimPDE.DeepBSDE","text":"DeepBSDE(u0,σᵀ∇u;opt=Flux.Optimise.Adam(0.1))\n\nDeepBSDE algorithm, from J. Han, A. Jentzen and Weinan E. \n\nArguments\n\nu0: a Flux.jl Chain with a d-dimensional input and a 1-dimensional output for the solytion guess.\nσᵀ∇u: a Flux.jl Chain for the BSDE value guess.\nopt: the optimization algorithm to be used to optimize the neural networks. Defaults to Flux.Optimise.Adam(0.1).\n\nExample\n\nBlack-Scholes-Barenblatt equation\n\nd = 30 # number of dimensions\nx0 = repeat([1.0f0, 0.5f0], div(d,2))\ntspan = (0.0f0,1.0f0)\ndt = 0.2\nm = 30 # number of trajectories (batch size)\n\nr = 0.05f0\nsigma = 0.4f0\nf(X,u,σᵀ∇u,p,t) = r * (u - sum(X.*σᵀ∇u))\ng(X) = sum(X.^2)\nμ_f(X,p,t) = zero(X) #Vector d x 1\nσ_f(X,p,t) = Diagonal(sigma*X) #Matrix d x d\nprob = PIDEProblem(μ_f, σ_f, x0, tspan, g, f)\n\nhls  = 10 + d #hidden layer size\nopt = Flux.Optimise.Adam(0.001)\nu0 = Flux.Chain(Dense(d,hls,relu),\n                Dense(hls,hls,relu),\n                Dense(hls,1))\nσᵀ∇u = Flux.Chain(Dense(d+1,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,hls,relu),\n                  Dense(hls,d))\npdealg = DeepBSDE(u0, σᵀ∇u, opt=opt)\n\nsolve(prob, \n    pdealg, \n    EM(), \n    verbose=true, \n    maxiters=150, \n    trajectories=m, \n    sdealg=StochasticDiffEq., \n    dt=dt, \n    pabstol = 1f-6)\n\n\n\n\n\n","category":"type"},{"location":"DeepBSDE/#CommonSolve.solve-Tuple{ParabolicPDEProblem, DeepBSDE, Any}","page":"The DeepBSDE algorithm","title":"CommonSolve.solve","text":"solve(\n    prob::ParabolicPDEProblem,\n    pdealg::DeepBSDE,\n    sdealg;\n    verbose,\n    maxiters,\n    trajectories,\n    dt,\n    pabstol,\n    save_everystep,\n    limits,\n    ensemblealg,\n    trajectories_upper,\n    trajectories_lower,\n    maxiters_limits,\n    kwargs...\n) -> Any\n\n\nReturns a PIDESolution object.\n\nArguments\n\nsdealg: a SDE solver from DifferentialEquations.jl.    If not provided, the plain vanilla DeepBSDE method will be applied.   If provided, the SDE associated with the PDE problem will be solved relying on    methods from DifferentialEquations.jl, using Ensemble solves    via sdealg. Check the available sdealg on the    DifferentialEquations.jl doc.\nlimits: if true, upper and lower limits will be calculated, based on    Deep Primal-Dual algorithm for BSDEs.\nmaxiters: The number of training epochs. Defaults to 300\ntrajectories: The number of trajectories simulated for training. Defaults to 100\nExtra keyword arguments passed to solve will be further passed to the SDE solver.\n\n\n\n\n\n","category":"method"},{"location":"DeepBSDE/#CommonSolve.solve-Tuple{ParabolicPDEProblem, DeepBSDE}","page":"The DeepBSDE algorithm","title":"CommonSolve.solve","text":"solve(\n    prob::ParabolicPDEProblem,\n    alg::DeepBSDE;\n    dt,\n    abstol,\n    verbose,\n    maxiters,\n    save_everystep,\n    trajectories,\n    ensemblealg,\n    limits,\n    trajectories_upper,\n    trajectories_lower,\n    maxiters_limits\n)\n\n\nReturns a PIDESolution object. \n\nArguments:\n\nmaxiters: The number of training epochs. Defaults to 300\ntrajectories: The number of trajectories simulated for training. Defaults to 100\n\nTo use SDE Algorithms use DeepBSDE\n\n\n\n\n\n","category":"method"},{"location":"DeepBSDE/#The-general-idea","page":"The DeepBSDE algorithm","title":"The general idea 💡","text":"","category":"section"},{"location":"DeepBSDE/","page":"The DeepBSDE algorithm","title":"The DeepBSDE algorithm","text":"The DeepBSDE algorithm is similar in essence to the DeepSplitting algorithm, with the difference that it uses two neural networks to approximate both the the solution and its gradient.","category":"page"},{"location":"DeepBSDE/#References","page":"The DeepBSDE algorithm","title":"References","text":"","category":"section"},{"location":"DeepBSDE/","page":"The DeepBSDE algorithm","title":"The DeepBSDE algorithm","text":"Han, J., Jentzen, A., E, W., Solving high-dimensional partial differential equations using deep learning. arXiv (2018)","category":"page"},{"location":"tutorials/nnkolmogorov/#NNKolmogorov","page":"NNKolmogorov","title":"NNKolmogorov","text":"","category":"section"},{"location":"tutorials/nnkolmogorov/#Solving-high-dimensional-Rainbow-European-Options-for-a-range-of-initial-stock-prices:","page":"NNKolmogorov","title":"Solving high dimensional Rainbow European Options for a range of initial stock prices:","text":"","category":"section"},{"location":"tutorials/nnkolmogorov/","page":"NNKolmogorov","title":"NNKolmogorov","text":"using HighDimPDE, StochasticDiffEq, Flux, LinearAlgebra\n\nd = 10 # dims\nT = 1 / 12\nsigma = 0.01 .+ 0.03 .* Matrix(Diagonal(ones(d))) # volatility \nmu = 0.06 # interest rate\nK = 100.0 # strike price\nfunction μ_func(du, u, p, t)\n    du .= mu * u\nend\n\nfunction σ_func(du, u, p, t)\n    du .= sigma * u\nend\n\ntspan = (0.0, T)\n# The range for initial stock price\nxspan = [(98.00, 102.00) for i in 1:d]\n\ng(x) = max(maximum(x) - K, 0)\n\nsdealg = SRIW1()\n# provide `x0` as nothing to the problem since we are provinding a range for `x0`.\nprob = ParabolicPDEProblem(μ_func, σ_func, nothing, tspan, g = g, xspan = xspan)\nopt = Flux.Optimisers.Adam(0.01)\nm = Chain(Dense(d, 16, elu), Dense(16, 32, elu), Dense(32, 16, elu), Dense(16, 1))\nalg = NNKolmogorov(m, opt)\nsol = solve(prob, alg, sdealg, verbose = true, dt = 0.01,\n    dx = 0.0001, trajectories = 1000, abstol = 1e-6, maxiters = 300)","category":"page"},{"location":"DeepSplitting/#deepsplitting","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"","category":"section"},{"location":"DeepSplitting/#Problems-Supported:","page":"The DeepSplitting algorithm","title":"Problems Supported:","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"PIDEProblem\nParabolicPDEProblem","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Modules = [HighDimPDE]\nPages   = [\"DeepSplitting.jl\"]","category":"page"},{"location":"DeepSplitting/#HighDimPDE.DeepSplitting","page":"The DeepSplitting algorithm","title":"HighDimPDE.DeepSplitting","text":"DeepSplitting(nn, K=1, opt = Flux.Optimise.Adam(0.01), λs = nothing, mc_sample =  NoSampling())\n\nDeep splitting algorithm.\n\nArguments\n\nnn: a Flux.Chain, or more generally a functor.\nK: the number of Monte Carlo integrations.\nopt: optimizer to be used. By default, Flux.Optimise.Adam(0.01).\nλs: the learning rates, used sequentially. Defaults to a single value taken from opt.\nmc_sample::MCSampling : sampling method for Monte Carlo integrations of the non-local term. Can be UniformSampling(a,b), NormalSampling(σ_sampling, shifted), or NoSampling (by default).\n\nExample\n\nhls = d + 50 # hidden layer size\nd = 10 # size of the sample\n\n# Neural network used by the scheme\nnn = Flux.Chain(Dense(d, hls, tanh),\n                Dense(hls,hls,tanh),\n                Dense(hls, 1, x->x^2))\n\nalg = DeepSplitting(nn, K=10, opt = Flux.Optimise.Adam(), λs = [5e-3,1e-3],\n                    mc_sample = UniformSampling(zeros(d), ones(d)) )\n\n\n\n\n\n","category":"type"},{"location":"DeepSplitting/#CommonSolve.solve-Tuple{Union{PIDEProblem, ParabolicPDEProblem}, DeepSplitting, Any}","page":"The DeepSplitting algorithm","title":"CommonSolve.solve","text":"solve(\n    prob::Union{PIDEProblem, ParabolicPDEProblem},\n    alg::DeepSplitting,\n    dt;\n    batch_size,\n    abstol,\n    verbose,\n    maxiters,\n    use_cuda,\n    cuda_device,\n    verbose_rate\n) -> PIDESolution{_A, _B, _C, Vector{_A1}, Vector{Any}, Nothing} where {_A, _B, _C, _A1}\n\n\nReturns a PIDESolution object.\n\nArguments\n\nmaxiters: number of iterations per time step. Can be a tuple, where maxiters[1] is used for the training of the neural network used in the first time step (which can be long) and maxiters[2] is used for the rest of the time steps.\nbatch_size : the batch size.\nabstol : threshold for the objective function under which the training is stopped.\nverbose : print training information.\nverbose_rate : rate for printing training information (every verbose_rate iterations).\nuse_cuda : set to true to use CUDA.\ncuda_device : integer, to set the CUDA device used in the training, if use_cuda == true.\n\n\n\n\n\n","category":"method"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The DeepSplitting algorithm reformulates the PDE as a stochastic learning problem.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The algorithm relies on two main ideas:","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The approximation of the solution u by a parametric function bf u^theta. This function is generally chosen as a (Feedforward) Neural Network, as it is a universal approximator.\nThe training of bf u^theta by simulated stochastic trajectories of particles, through the link between linear PDEs and the expected trajectory of associated Stochastic Differential Equations (SDEs), explicitly stated by the Feynman Kac formula.","category":"page"},{"location":"DeepSplitting/#The-general-idea","page":"The DeepSplitting algorithm","title":"The general idea 💡","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Consider the PDE","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"partial_t u(tx) = mu(t x) nabla_x u(tx) + frac12 sigma^2(t x) Delta_x u(tx) + f(x u(tx)) tag1","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"with initial conditions u(0 x) = g(x), where u colon R^d to R.","category":"page"},{"location":"DeepSplitting/#Local-Feynman-Kac-formula","page":"The DeepSplitting algorithm","title":"Local Feynman Kac formula","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"DeepSplitting solves the PDE iteratively over small time intervals by using an approximate Feynman-Kac representation locally.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"More specifically, considering a small time step dt = t_n+1 - t_n one has that","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"u(t_n+1 X_T - t_n+1) approx mathbbE left f(t X_T - t_n u(t_nX_T - t_n))(t_n+1 - t_n) + u(t_n X_T - t_n)  X_T - t_n+1right tag3","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"One can therefore use Monte Carlo integrations to approximate the expectations","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"u(t_n+1 X_T - t_n+1) approx frac1textbatch_sizesum_j=1^textbatch_size left u(t_n X_T - t_n^(j)) + (t_n+1 - t_n)sum_k=1^K big f(t_n X_T - t_n^(j) u(t_nX_T - t_n^(j))) big right","category":"page"},{"location":"DeepSplitting/#Reformulation-as-a-learning-problem","page":"The DeepSplitting algorithm","title":"Reformulation as a learning problem","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The DeepSplitting algorithm approximates u(t_n+1 x) by a parametric function bf u^theta_n(x). It is advised to let this function be a neural network bf u_theta equiv NN_theta as they are universal approximators.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"For each time step t_n, the DeepSplitting algorithm","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Generates the particle trajectories X^x (j) satisfying Eq. (2) over the whole interval 0T.\nSeeks bf u_n+1^theta  by minimizing the loss function","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"L(theta) = bf u^theta_n+1(X_T - t_n) - left f(t X_T - t_n-1 bf u_n-1(X_T - t_n-1))(t_n - t_n-1) + bf u_n-1(X_T - t_n-1) right ","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"This way, the PDE approximation problem is decomposed into a sequence of separate learning problems. In HighDimPDE.jl the right parameter combination theta is found by iteratively minimizing L using stochastic gradient descent.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"tip: Tip\nTo solve with DeepSplitting, one needs to provide to solvedt\nbatch_size\nmaxiters: the number of iterations for minimizing the loss function\nabstol: the absolute tolerance for the loss function\nuse_cuda: if you have a Nvidia GPU, recommended.","category":"page"},{"location":"DeepSplitting/#Solving-point-wise-or-on-a-hypercube","page":"The DeepSplitting algorithm","title":"Solving point-wise or on a hypercube","text":"","category":"section"},{"location":"DeepSplitting/#Pointwise","page":"The DeepSplitting algorithm","title":"Pointwise","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"DeepSplitting allows obtaining u(tx) on a single point  x in Omega with the keyword x.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"prob = PIDEProblem(μ, σ, x, tspan, g, f)","category":"page"},{"location":"DeepSplitting/#Hypercube","page":"The DeepSplitting algorithm","title":"Hypercube","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Yet more generally, one wants to solve Eq. (1) on a d-dimensional cube ab^d. This is offered by HighDimPDE.jl with the keyword x0_sample.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"prob = PIDEProblem(μ, σ, x, tspan, g, f; x0_sample = x0_sample)","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Internally, this is handled by assigning a random variable as the initial point of the particles, i.e.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"X_t^xi = int_0^t mu(X_s^x)ds + int_0^tsigma(X_s^x)dB_s + xi","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"where xi a random variable uniformly distributed over ab^d. This way, the neural network is trained on the whole interval ab^d instead of a single point.","category":"page"},{"location":"DeepSplitting/#Non-local-PDEs","page":"The DeepSplitting algorithm","title":"Non-local PDEs","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"DeepSplitting can solve for non-local reaction diffusion equations of the type","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"partial_t u = mu(x) nabla_x u + frac12 sigma^2(x) Delta u + int_Omegaf(xy u(tx) u(ty))dy","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The non-localness is handled by a Monte Carlo integration.","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"u(t_n+1 X_T - t_n+1) approx sum_j=1^textbatch_size left u(t_n X_T - t_n^(j)) + frac(t_n+1 - t_n)Ksum_k=1^K big f(t X_T - t_n^(j) Y_X_T - t_n^(j)^(k) u(t_nX_T - t_n^(j)) u(t_nY_X_T - t_n^(j)^(k))) big right","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"tip: Tip\n","category":"page"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"In practice, if you have a non-local model, you need to provide the sampling method and the number K of MC integration through the keywords mc_sample and K. julia alg = DeepSplitting(nn, opt = opt, mc_sample = mc_sample, K = 1) mc_sample can be whether UniformSampling(a, b) or NormalSampling(σ_sampling, shifted).","category":"page"},{"location":"DeepSplitting/#References","page":"The DeepSplitting algorithm","title":"References","text":"","category":"section"},{"location":"DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Boussange, V., Becker, S., Jentzen, A., Kuckuck, B., Pellissier, L., Deep learning approximations for non-local nonlinear PDEs with Neumann boundary conditions. arXiv (2022)\nBeck, C., Becker, S., Cheridito, P., Jentzen, A., Neufeld, A., Deep splitting method for parabolic PDEs. arXiv (2019)","category":"page"},{"location":"MLP/#mlp","page":"The MLP algorithm","title":"The MLP algorithm","text":"","category":"section"},{"location":"MLP/#Problems-Supported:","page":"The MLP algorithm","title":"Problems Supported:","text":"","category":"section"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"PIDEProblem\nParabolicPDEProblem","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Modules = [HighDimPDE]\nPages   = [\"MLP.jl\"]","category":"page"},{"location":"MLP/#HighDimPDE.MLP","page":"The MLP algorithm","title":"HighDimPDE.MLP","text":"Multi level Picard algorithm.\n\nArguments\n\nL: number of Picard iterations (Level),\nM: number of Monte Carlo integrations (at each level l, M^(L-l)integrations),\nK: number of Monte Carlo integrations for the non-local term\nmc_sample::MCSampling : sampling method for Monte Carlo integrations of the non local term.\n\nCan be UniformSampling(a,b), NormalSampling(σ_sampling), or NoSampling (by default).\n\n\n\n\n\n","category":"type"},{"location":"MLP/#CommonSolve.solve-Tuple{Union{PIDEProblem, ParabolicPDEProblem}, MLP}","page":"The MLP algorithm","title":"CommonSolve.solve","text":"solve(\n    prob::Union{PIDEProblem, ParabolicPDEProblem},\n    alg::MLP;\n    multithreading,\n    verbose\n) -> PIDESolution{_A, _B, Nothing, _C, Nothing, Nothing} where {_A, _B, _C}\n\n\nReturns a PIDESolution object.\n\nArguments\n\nmultithreading : if true, distributes the job over all the threads available.\nverbose: print information over the iterations.\n\n\n\n\n\n","category":"method"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The MLP, for Multi-Level Picard iterations, reformulates the PDE problem as a fixed point equation through the Feynman Kac formula.","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"It relies on Picard iterations to find the fixed point,\nreducing the complexity of the numerical approximation of the time integral through a multilevel Monte Carlo approach.","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The MLP algorithm overcomes the curse of dimensionality, with a computational complexity that grows polynomially in the number of dimension (see M. Hutzenthaler et al. 2020).","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"warning: `MLP` can only approximate the solution on a single point\nMLP only works for PIDEProblem with x0_sample = NoSampling(). If you want to solve over an entire domain, you definitely want to check the DeepSplitting algorithm.","category":"page"},{"location":"MLP/#The-general-idea","page":"The MLP algorithm","title":"The general idea 💡","text":"","category":"section"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Consider the PDE","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"partial_t u(tx) = mu(t x) nabla_x u(tx) + frac12 sigma^2(t x) Delta_x u(tx) + f(x u(tx)) tag1","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"with initial conditions u(0 x) = g(x), where u colon R^d to R.","category":"page"},{"location":"MLP/#Picard-Iterations","page":"The MLP algorithm","title":"Picard Iterations","text":"","category":"section"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The MLP algorithm observes that the Feynman Kac formula can be viewed as a fixed point equation, i.e. u = phi(u). Introducing a sequence (u_k) defined as u_0 = g and","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"u_l+1 = phi(u_l)","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"the Banach fixed-point theorem ensures that the sequence converges to the true solution u. Such a technique is known as Picard iterations.","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The time integral term is evaluated by a Monte-Carlo integration","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"u_L  = frac1Msum_i^M left f(X^x(i)_t - s_(l i) u_L-1(T-s_i X^x( i)_t - s_(l i))) + u(0 X^x(i)_t - s_(l i)) right","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"But the MLP uses an extra trick to lower the computational cost of the iteration.","category":"page"},{"location":"MLP/#Telescope-sum","page":"The MLP algorithm","title":"Telescope sum","text":"","category":"section"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The MLP algorithm uses a telescope sum","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"beginaligned\nu_L = phi(u_L-1) = phi(u_L-1) - phi(u_L-2) + phi(u_L-2) - phi(u_L-3) + dots \n= sum_l=1^L-1 phi(u_l-1) - phi(u_l-2)\nendaligned","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"As l grows, the term phi(u_l-1) - phi(u_l-2) becomes smaller, and thus demands more calculations. The MLP algorithm uses this fact by evaluating the integral term at level l with M^L-l samples.","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"tip: Tip\nL corresponds to the level of the approximation, i.e. u approx u_L\nM characterizes the number of samples for the Monte Carlo approximation of the time integral","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Overall, MLP can be summarized by the following formula","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"beginaligned\nu_L = sum_l=1^L-1 frac1M^L-lsum_i^M^L-l left f(X^x(l i)_t - s_(l i) u(T-s_(l i) X^x(l i)_t - s_(l i))) + mathbf1_N(l) f(X^x(l i)_t - s_(l i) u(T-s_(l i) X^x(l i)_t - s_(l i)))right\n\nqquad + frac1M^Lsum_i^M^L u(0 X^x(l i)_t)\nendaligned","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Note that the superscripts (l i) indicate the independence of the random variables X across levels.","category":"page"},{"location":"MLP/#Non-local-PDEs","page":"The MLP algorithm","title":"Non-local PDEs","text":"","category":"section"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"MLP can solve for non-local reaction diffusion equations of the type","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"partial_t u = mu(t x) nabla_x u(t x) + frac12 sigma^2(t x) Delta u(t x) + int_Omegaf(x y u(tx) u(ty))dy","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The non-localness is handled by a Monte Carlo integration.","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"beginaligned\nu_L = sum_l=1^L-1 frac1M^L-lsum_i=1^M^L-l frac1Ksum_j=1^K  bigg f(X^x(l i)_t - s_(l i) Z^(lj) u(T-s_(l i) X^x(l i)_t - s_(l i)) u(T-s_li Z^(lj))) + \nqquad \nmathbf1_N(l) f(X^x(l i)_t - s_(l i) u(T-s_(l i) X^x(l i)_t - s_(l i)))bigg + frac1M^Lsum_i^M^L u(0 X^x(l i)_t)\nendaligned","category":"page"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"tip: Tip\nIn practice, if you have a non-local model, you need to provide the sampling method and the number K of MC integration through the keywords mc_sample and K.K characterizes the number of samples for the Monte Carlo approximation of the last term.\nmc_sample characterizes the distribution of the Z variables","category":"page"},{"location":"MLP/#References","page":"The MLP algorithm","title":"References","text":"","category":"section"},{"location":"MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Boussange, V., Becker, S., Jentzen, A., Kuckuck, B., Pellissier, L., Deep learning approximations for non-local nonlinear PDEs with Neumann boundary conditions. arXiv (2022)\nBecker, S., Braunwarth, R., Hutzenthaler, M., Jentzen, A., von Wurstemberger, P., Numerical simulations for full history recursive multilevel Picard approximations for systems of high-dimensional partial differential equations. arXiv (2020)","category":"page"},{"location":"Feynman_Kac/#feynmankac","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"","category":"section"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"The Feynman Kac formula is generally stated for terminal condition problems (see e.g. Wikipedia), where","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"partial_t u(tx) + mu(x) nabla_x u(tx) + frac12 sigma^2(x) Delta_x u(tx) + f(x u(tx))  = 0 tag1","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"with terminal condition u(T x) = g(x), and u colon R^d to R.","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"In this case, the FK formula states that for all t in (0T) it holds that","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"u(t x) = int_t^T mathbbE left f(X^x_s-t u(s X^x_s-t))ds right + mathbbE left u(0 X^x_T-t) right tag2","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"where","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"X_t^x = int_0^t mu(X_s^x)ds + int_0^tsigma(X_s^x)dB_s + x","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"and B_t is a Brownian motion.","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"(Image: Brownian motion - Wikipedia)","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"Intuitively, this formula is motivated by the fact that the density of Brownian particles (motion) satisfies the diffusion equation.","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"The equivalence between the average trajectory of particles and PDEs given by the Feynman-Kac formula allows overcoming the curse of dimensionality that standard numerical methods suffer from, because the expectations can be approximated Monte Carlo integrations, which approximation error decreases as 1sqrtN and is therefore not dependent on the dimensions. On the other hand, the computational complexity of traditional deterministic techniques grows exponentially in the number of dimensions.","category":"page"},{"location":"Feynman_Kac/#Forward-non-linear-Feynman-Kac","page":"Feynman Kac formula","title":"Forward non-linear Feynman-Kac","text":"","category":"section"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"How to transform previous equation to an initial value problem?","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"Define v(tau x) = u(T-tau x). Observe that v(0x) = u(Tx). Further, observe that by the chain rule","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"beginaligned\npartial_tau v(tau x) = partial_tau u(T-taux)\n                        = (partial_tau (T-tau)) partial_t u(T-taux)\n                        = -partial_t u(T-tau x)\nendaligned","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"From Eq. (1) we get that","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"- partial_t u(T - taux) = mu(x) nabla_x u(T - taux) + frac12 sigma^2(x) Delta_x u(T - taux) + f(x u(T - taux))","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"Replacing  u(T-tau x) by v(tau x) we get that v satisfies","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"partial_tau v(tau x) = mu(x) nabla_x v(taux) + frac12 sigma^2(x) Delta_x v(taux) + f(x v(taux)) ","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"and from Eq. (2) we obtain","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"v(tau x) = int_T-tau^T mathbbE left f(X^x_s- T + tau v(s X^x_s-T + tau))ds right + mathbbE left v(0 X^x_tau) right","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"By using the substitution rule with tau to tau -T (shifting by T) and tau to - tau (inversing), and finally inversing the integral bound we get that","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"beginaligned\nv(tau x) = int_-tau^0 mathbbE left f(X^x_s + tau v(s + T X^x_s + tau))ds right + mathbbE left v(0 X^x_tau) right\n            = - int_tau^0 mathbbE left f(X^x_tau - s v(T-s X^x_tau - s))ds right + mathbbE left v(0 X^x_tau) right\n            = int_0^tau mathbbE left f(X^x_tau - s v(T-s X^x_tau - s))ds right + mathbbE left v(0 X^x_tau) right\nendaligned","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"This leads to the","category":"page"},{"location":"Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"info: Non-linear Feynman Kac for initial value problems\nConsider the PDEpartial_t u(tx) = mu(t x) nabla_x u(tx) + frac12 sigma^2(t x) Delta_x u(tx) + f(x u(tx))with initial conditions u(0 x) = g(x), where u colon R^d to R. Thenu(t x) = int_0^t mathbbE left f(X^x_t - s u(T-s X^x_t - s))ds right + mathbbE left u(0 X^x_t) right tag3withX_t^x = int_0^t mu(X_s^x)ds + int_0^tsigma(X_s^x)dB_s + x","category":"page"},{"location":"NNParamKolmogorov/#nn_paramkolmogorov","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"","category":"section"},{"location":"NNParamKolmogorov/#Problems-Supported:","page":"The NNParamKolmogorov algorithm","title":"Problems Supported:","text":"","category":"section"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"ParabolicPDEProblem","category":"page"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"Modules = [HighDimPDE]\nPages   = [\"NNParamKolmogorov.jl\"]","category":"page"},{"location":"NNParamKolmogorov/#HighDimPDE.NNParamKolmogorov","page":"The NNParamKolmogorov algorithm","title":"HighDimPDE.NNParamKolmogorov","text":"Algorithm for solving paramateric families of Kolmogorov Equations.\n\nHighDimPDE.NNKolmogorov(chain, opt)\n\nArguments:\n\nchain: A Chain neural network with a d-dimensional output.\nopt: The optimizer to train the neural network. Defaults to ADAM(0.1).\n\n[1] Berner Julius et al. \"Numerically solving parametric families of high-dimensional Kolmogorov partial differential equations via deep learning.\"\n\n\n\n\n\n","category":"type"},{"location":"NNParamKolmogorov/#CommonSolve.solve","page":"The NNParamKolmogorov algorithm","title":"CommonSolve.solve","text":"solve(\n    prob::ParabolicPDEProblem,\n    pdealg::NNParamKolmogorov;\n    ...\n)\nsolve(\n    prob::ParabolicPDEProblem,\n    pdealg::NNParamKolmogorov,\n    sdealg;\n    ensemblealg,\n    abstol,\n    verbose,\n    maxiters,\n    trajectories,\n    save_everystep,\n    use_gpu,\n    dps,\n    dt,\n    dx,\n    kwargs...\n)\n\n\nReturns a PIDESolution object.\n\nArguments\n\nsdealg: a SDE solver from DifferentialEquations.jl.    If not provided, the plain vanilla DeepBSDE method will be applied.   If provided, the SDE associated with the PDE problem will be solved relying on    methods from DifferentialEquations.jl, using Ensemble solves    via sdealg. Check the available sdealg on the    DifferentialEquations.jl doc.\nmaxiters: The number of training epochs. Defaults to 300\ntrajectories: The number of trajectories simulated for training. Defaults to 100\ndps::NamedTuple: The sampling interval for ranges of parameters. Should have keys : p_sigma, 'pmuandpphi`\nExtra keyword arguments passed to solve will be further passed to the SDE solver.\n\n\n\n\n\n","category":"function"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"NNParamKolmogorov obtains a","category":"page"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"terminal solution for parametric families of Forward Kolmogorov Equations of the form:","category":"page"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"partial_t u(tx) = mu(t x γ_mu) nabla_x u(tx) + frac12 sigma^2(t x γ_sigma) Delta_x u(tx)","category":"page"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"with initial condition given by g(x, γ_phi)","category":"page"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"or an initial condition for parametric families of Backward Kolmogorov Equations of the form:","category":"page"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"partial_t u(tx) = - mu(t x) nabla_x u(tx) - frac12 sigma^2(t x) Delta_x u(tx)","category":"page"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"with terminal condition given by g(x, γ_phi)","category":"page"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"We can use the Feynman-Kac formula :","category":"page"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"S_t^x = int_0^tmu(S_s^x)ds + int_0^tsigma(S_s^x)dB_s","category":"page"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"And the solution is given by:","category":"page"},{"location":"NNParamKolmogorov/","page":"The NNParamKolmogorov algorithm","title":"The NNParamKolmogorov algorithm","text":"f(T x) = mathbbEg(S_T^x γ_phi)","category":"page"},{"location":"tutorials/nnparamkolmogorov/#NNParamKolmogorov","page":"NNParamKolmogorov","title":"NNParamKolmogorov","text":"","category":"section"},{"location":"tutorials/nnparamkolmogorov/#Solving-Parametric-Family-of-High-Dimensional-Heat-Equation.","page":"NNParamKolmogorov","title":"Solving Parametric Family of High Dimensional Heat Equation.","text":"","category":"section"},{"location":"tutorials/nnparamkolmogorov/","page":"NNParamKolmogorov","title":"NNParamKolmogorov","text":"In this example we will solve the high dimensional heat equation over a range of initial values, and also over a range of thermal diffusivity.","category":"page"},{"location":"tutorials/nnparamkolmogorov/","page":"NNParamKolmogorov","title":"NNParamKolmogorov","text":"using HighDimPDE, Flux, StochasticDiffEq\nd = 10\n# models input is `d` for initial values, `d` for thermal diffusivity, and last dimension is for stopping time.\nm = Chain(Dense(d + 1 + 1, 32, relu), Dense(32, 16, relu), Dense(16, 8, relu), Dense(8, 1))\nensemblealg = EnsembleThreads()\nγ_mu_prototype = nothing\nγ_sigma_prototype = zeros(1, 1)\nγ_phi_prototype = nothing\n\nsdealg = EM()\ntspan = (0.00, 1.00)\ntrajectories = 100000\nfunction phi(x, y_phi)\n    sum(x .^ 2)\nend\nfunction sigma_(dx, x, γ_sigma, t)\n    dx .= γ_sigma[:, :, 1]\nend\nmu_(dx, x, γ_mu, t) = dx .= 0.00\n\nxspan = [(0.00, 3.00) for i in 1:d]\n\np_domain = (p_sigma = (0.00, 2.00), p_mu = nothing, p_phi = nothing)\np_prototype = (p_sigma = γ_sigma_prototype, p_mu = γ_mu_prototype, p_phi = γ_phi_prototype)\ndps = (p_sigma = 0.1, p_mu = nothing, p_phi = nothing)\n\ndt = 0.01\ndx = 0.01\nopt = Flux.Optimisers.Adam(5e-2)\n\nprob = ParabolicPDEProblem(mu_,\n    sigma_,\n    nothing,\n    tspan;\n    g = phi,\n    xspan,\n    p_domain = p_domain,\n    p_prototype = p_prototype)\n\nsol = solve(prob, NNParamKolmogorov(m, opt), sdealg, verbose = true, dt = 0.01,\n    abstol = 1e-10, dx = 0.1, trajectories = trajectories, maxiters = 1000,\n    use_gpu = false, dps = dps)","category":"page"},{"location":"tutorials/nnparamkolmogorov/","page":"NNParamKolmogorov","title":"NNParamKolmogorov","text":"Similarly we can parametrize the drift function mu_ and the initial function g, and obtain a solution over all parameters and initial values.","category":"page"},{"location":"tutorials/nnparamkolmogorov/#Inferring-on-the-solution-from-NNParamKolmogorov:","page":"NNParamKolmogorov","title":"Inferring on the solution from NNParamKolmogorov:","text":"","category":"section"},{"location":"tutorials/nnparamkolmogorov/","page":"NNParamKolmogorov","title":"NNParamKolmogorov","text":"x_test = rand(xspan[1][1]:0.1:xspan[1][2], d)\np_sigma_test = rand(p_domain.p_sigma[1]:(dps.p_sigma):p_domain.p_sigma[2], 1, 1)\nt_test = rand(tspan[1]:dt:tspan[2], 1, 1)\np_mu_test = nothing\np_phi_test = nothing","category":"page"},{"location":"tutorials/nnparamkolmogorov/","page":"NNParamKolmogorov","title":"NNParamKolmogorov","text":"sol.ufuns(x_test, t_test, p_sigma_test, p_mu_test, p_phi_test)","category":"page"},{"location":"NNStopping/#nn_stopping","page":"The NNStopping algorithm","title":"The NNStopping algorithm","text":"","category":"section"},{"location":"NNStopping/#Problems-Supported:","page":"The NNStopping algorithm","title":"Problems Supported:","text":"","category":"section"},{"location":"NNStopping/","page":"The NNStopping algorithm","title":"The NNStopping algorithm","text":"ParabolicPDEProblem","category":"page"},{"location":"NNStopping/","page":"The NNStopping algorithm","title":"The NNStopping algorithm","text":"Modules = [HighDimPDE]\nPages   = [\"NNStopping.jl\"]","category":"page"},{"location":"NNStopping/#HighDimPDE.NNStopping","page":"The NNStopping algorithm","title":"HighDimPDE.NNStopping","text":"NNStopping(models, opt)\n\nDeep Optimal Stopping, S Becker, P Cheridito, A Jentzen3, and T Welti.\n\nArguments\n\nmodels::Vector{Flux.Chain}: A vector of Flux.Chain where each model corresponds to a specific timestep from the timespan (tspan). The overall length of the vector should be length(timesteps) - 1.\nopt: the optimization algorithm to be used to optimize the neural networks. Defaults to ADAM(0.1).\n\nExample\n\nd = 3 # Number of assets in the stock r = 0.05 # interest rate beta = 0.2 # volatility T = 3 # maturity u0 = fill(90.0, d) # initial stock value delta = 0.1 # delta f(du, u, p, t) = du .= (r - delta) * u # drift sigma(du, u, p, t) = du .= beta * u # diffusion tspan = (0.0, T) N = 9 # discretization parameter dt = T / (N) K = 100.00 # strike price\n\npayoff function\n\nfunction g(x, t)     return exp(-r * t) * (max(maximum(x) - K, 0)) end\n\nprob = PIDEProblem(f, sigma, u0, tspan; payoff = g) models = [Chain(Dense(d + 1, 32, tanh), BatchNorm(32, tanh), Dense(32, 1, sigmoid))           for i in 1:N]\n\nopt = Flux.Optimisers.Adam(0.01) alg = NNStopping(models, opt)\n\nsol = solve(prob, alg, SRIW1(); dt = dt, trajectories = 1000, maxiters = 1000, verbose = true) ```\n\n\n\n\n\n","category":"type"},{"location":"NNStopping/#CommonSolve.solve-Tuple{ParabolicPDEProblem, NNStopping, Any}","page":"The NNStopping algorithm","title":"CommonSolve.solve","text":"solve(\n    prob::ParabolicPDEProblem,\n    pdealg::NNStopping,\n    sdealg;\n    verbose,\n    maxiters,\n    trajectories,\n    dt,\n    ensemblealg,\n    kwargs...\n) -> NamedTuple{(:payoff, :stopping_time), <:Tuple{Any, Any}}\n\n\nReturns a NamedTuple with payoff and stopping_time\n\nArguments: \n\nsdealg: a SDE solver from DifferentialEquations.jl.    If not provided, the plain vanilla DeepBSDE method will be applied.   If provided, the SDE associated with the PDE problem will be solved relying on    methods from DifferentialEquations.jl, using Ensemble solves    via sdealg. Check the available sdealg on the    DifferentialEquations.jl doc.\nmaxiters: The number of training epochs. Defaults to 300\ntrajectories: The number of trajectories simulated for training. Defaults to 100\nExtra keyword arguments passed to solve will be further passed to the SDE solver.\n\n\n\n\n\n","category":"method"},{"location":"NNStopping/#The-general-idea","page":"The NNStopping algorithm","title":"The general idea 💡","text":"","category":"section"},{"location":"NNStopping/","page":"The NNStopping algorithm","title":"The NNStopping algorithm","text":"Similar to DeepSplitting and DeepBSDE, NNStopping evaluates the PDE as a Stochastic Differential Equation. Consider an Obstacle PDE of the form:","category":"page"},{"location":"NNStopping/","page":"The NNStopping algorithm","title":"The NNStopping algorithm","text":" maxlbracepartial_t u(tx) + mu(t x) nabla_x u(tx) + frac12 sigma^2(t x) Delta_x u(tx)  g(tx) - u(tx)rbrace","category":"page"},{"location":"NNStopping/","page":"The NNStopping algorithm","title":"The NNStopping algorithm","text":"Such PDEs are commonly used as representations for the dynamics of stock prices that can be exercised before maturity, such as American Options.","category":"page"},{"location":"NNStopping/","page":"The NNStopping algorithm","title":"The NNStopping algorithm","text":"Using the Feynman-Kac formula, the underlying SDE will be:","category":"page"},{"location":"NNStopping/","page":"The NNStopping algorithm","title":"The NNStopping algorithm","text":"dX_t=mu(Xt)dt + sigma(Xt) dW_t^Q","category":"page"},{"location":"NNStopping/","page":"The NNStopping algorithm","title":"The NNStopping algorithm","text":"The payoff of the option would then be:","category":"page"},{"location":"NNStopping/","page":"The NNStopping algorithm","title":"The NNStopping algorithm","text":"suplbracemathbbEg(X_tau tau)rbrace","category":"page"},{"location":"NNStopping/","page":"The NNStopping algorithm","title":"The NNStopping algorithm","text":"Where τ is the stopping (exercising) time. The goal is to retrieve both the optimal exercising strategy (τ) and the payoff.","category":"page"},{"location":"NNStopping/","page":"The NNStopping algorithm","title":"The NNStopping algorithm","text":"We approximate each stopping decision with a neural network architecture, inorder to maximise the expected payoff.","category":"page"},{"location":"getting_started/#Getting-started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"getting_started/#General-workflow","page":"Getting started","title":"General workflow","text":"","category":"section"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"The general workflow for using HighDimPDE.jl is as follows:","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"Define a Partial Integro-Differential Equation problem\nSelect a solver algorithm\nSolve the problem.","category":"page"},{"location":"getting_started/#Examples","page":"Getting started","title":"Examples","text":"","category":"section"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"Let's illustrate that with some examples.","category":"page"},{"location":"getting_started/#MLP","page":"Getting started","title":"MLP","text":"","category":"section"},{"location":"getting_started/#Local-PDE","page":"Getting started","title":"Local PDE","text":"","category":"section"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"Let's solve the Fisher KPP PDE in dimension 10 with MLP.","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"partial_t u = u (1 - u) + frac12sigma^2Delta_xu tag1","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"using HighDimPDE\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0, 0.5) # time horizon\nx0 = fill(0.0, d)  # initial point\ng(x) = exp(-sum(x .^ 2)) # initial condition\nμ(x, p, t) = 0.0 # advection coefficients\nσ(x, p, t) = 0.1 # diffusion coefficients\nf(x, v_x, ∇v_x, p, t) = max(0.0, v_x) * (1 - max(0.0, v_x)) # nonlocal nonlinear part of the\nprob = ParabolicPDEProblem(μ, σ, x0, tspan; g, f) # defining the problem\n\n## Definition of the algorithm\nalg = MLP() # defining the algorithm. We use the Multi Level Picard algorithm\n\n## Solving with multiple threads \nsol = solve(prob, alg, multithreading = true)","category":"page"},{"location":"getting_started/#Non-local-PDE-with-Neumann-boundary-conditions","page":"Getting started","title":"Non-local PDE with Neumann boundary conditions","text":"","category":"section"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"Let's include in the previous equation non-local competition, i.e.","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"partial_t u = u (1 - int_Omega u(ty)dy) + frac12sigma^2Delta_xu tag2","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"where Omega = -12 12^d, and let's assume Neumann Boundary condition on Omega.","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"using HighDimPDE\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0, 0.5) # time horizon\nx0 = fill(0.0, d)  # initial point\ng(x) = exp(-sum(x .^ 2)) # initial condition\nμ(x, p, t) = 0.0 # advection coefficients\nσ(x, p, t) = 0.1 # diffusion coefficients\nmc_sample = UniformSampling(fill(-5.0f-1, d), fill(5.0f-1, d))\nf(x, y, v_x, v_y, ∇v_x, ∇v_y, p, t) = max(0.0, v_x) * (1 - max(0.0, v_y))\nprob = PIDEProblem(μ, σ, x0, tspan, g, f) # defining x0_sample is sufficient to implement Neumann boundary conditions\n\n## Definition of the algorithm\nalg = MLP(mc_sample = mc_sample)\n\nsol = solve(prob, alg, multithreading = true)","category":"page"},{"location":"getting_started/#DeepSplitting","page":"Getting started","title":"DeepSplitting","text":"","category":"section"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"Let's solve the previous equation with DeepSplitting.","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"using HighDimPDE\nusing Flux # needed to define the neural network\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0, 0.5) # time horizon\nx0 = fill(0.0f0, d)  # initial point\ng(x) = exp.(-sum(x .^ 2, dims = 1)) # initial condition\nμ(x, p, t) = 0.0f0 # advection coefficients\nσ(x, p, t) = 0.1f0 # diffusion coefficients\nx0_sample = UniformSampling(fill(-5.0f-1, d), fill(5.0f-1, d))\nf(x, y, v_x, v_y, ∇v_x, ∇v_y, p, t) = v_x .* (1.0f0 .- v_y)\nprob = PIDEProblem(μ, σ, x0, tspan, g, f;\n    x0_sample = x0_sample)\n\n## Definition of the neural network to use\nhls = d + 50 #hidden layer size\n\nnn = Flux.Chain(Dense(d, hls, tanh),\n    Dense(hls, hls, tanh),\n    Dense(hls, 1)) # neural network used by the scheme\n\nopt = ADAM(1e-2)\n\n## Definition of the algorithm\nalg = DeepSplitting(nn,\n    opt = opt,\n    mc_sample = x0_sample)\n\nsol = solve(prob,\n    alg,\n    0.1,\n    verbose = true,\n    abstol = 2e-3,\n    maxiters = 1000,\n    batch_size = 1000)","category":"page"},{"location":"getting_started/#Solving-on-the-GPU","page":"Getting started","title":"Solving on the GPU","text":"","category":"section"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"DeepSplitting can run on the GPU for (much) improved performance. To do so, just set use_cuda = true.","category":"page"},{"location":"getting_started/","page":"Getting started","title":"Getting started","text":"sol = solve(prob,\n    alg,\n    0.1,\n    verbose = true,\n    abstol = 2e-3,\n    maxiters = 1000,\n    batch_size = 1000,\n    use_cuda = true)","category":"page"},{"location":"problems/","page":"Problems","title":"Problems","text":"PIDEProblem\nParabolicPDEProblem","category":"page"},{"location":"problems/#HighDimPDE.PIDEProblem","page":"Problems","title":"HighDimPDE.PIDEProblem","text":"PIDEProblem(\n    μ,\n    σ,\n    x0,\n    tspan,\n    g,\n    f;\n    p,\n    x0_sample,\n    neumann_bc,\n    kw...\n)\n\n\nDefines a Partial Integro Differential Problem, of the form\n\nbeginaligned\n    fracdudt = tfrac12 textTr(sigma sigma^T) Delta u(x t) + mu nabla u(x t) \n    quad + int f(x y u(x t) u(y t) ( nabla_x u )(x t) ( nabla_x u )(y t) p t) dy\nendaligned\n\nwith u(x0) = g(x).\n\nArguments\n\ng : initial condition, of the form g(x, p, t).\nf : nonlinear function, of the form f(x, y, u(x, t), u(y, t), ∇u(x, t), ∇u(y, t), p, t).\nμ : drift function, of the form μ(x, p, t).\nσ : diffusion function σ(x, p, t).\nx: point where u(x,t) is approximated. Is required even in the case where x0_sample is provided. Determines the dimensionality of the PDE.\ntspan: timespan of the problem.\np: the parameter vector.\nx0_sample : sampling method for x0. Can be UniformSampling(a,b), NormalSampling(σ_sampling, shifted), or NoSampling (by default). If NoSampling, only solution at the single point x is evaluated.\nneumann_bc: if provided, Neumann boundary conditions on the hypercube neumann_bc[1] × neumann_bc[2].\n\n\n\n\n\n","category":"type"},{"location":"problems/#HighDimPDE.ParabolicPDEProblem","page":"Problems","title":"HighDimPDE.ParabolicPDEProblem","text":"ParabolicPDEProblem(\n    μ,\n    σ,\n    x0,\n    tspan;\n    g,\n    f,\n    p,\n    xspan,\n    x0_sample,\n    neumann_bc,\n    payoff,\n    kw...\n)\n\n\nDefines a Parabolic Partial Differential Equation of the form:\n\nbeginaligned\n    fracdudt = tfrac12 textTr(sigma sigma^T) Delta u(x t) + mu nabla u(x t) \n    quad +  f(x u(x t) ( nabla_x u )(x t) p t)\nendaligned\n\nSemilinear Parabolic Partial Differential Equation \nf -> f(X, u, σᵀ∇u, p, t)\nKolmogorov Differential Equation\nf -> nothing\nx0 -> nothing, xspan must be provided.\nObstacle Partial Differential Equation \nf -> nothing\ng -> nothing\ndiscounted payoff function provided.\n\nArguments\n\nμ : drift function, of the form μ(x, p, t).\nσ : diffusion function σ(x, p, t).\nx: point where u(x,t) is approximated. Is required even in the case where x0_sample is provided. Determines the dimensionality of the PDE.\ntspan: timespan of the problem.\ng : initial condition, of the form g(x, p, t).\nf : nonlinear function, of the form  f(X, u, σᵀ∇u, p, t)\n\nOptional Arguments\n\np: the parameter vector.\nx0_sample : sampling method for x0. Can be UniformSampling(a,b), NormalSampling(σ_sampling, shifted), or NoSampling (by default). If NoSampling, only solution at the single point x is evaluated.\nneumann_bc: if provided, Neumann boundary conditions on the hypercube neumann_bc[1] × neumann_bc[2].\nxspan: The domain of the independent variable x\npayoff: The discounted payoff function. Required when solving for optimal stopping problem (Obstacle PDEs).\n\n\n\n\n\n","category":"type"},{"location":"problems/","page":"Problems","title":"Problems","text":"note: Note\n","category":"page"},{"location":"problems/","page":"Problems","title":"Problems","text":"While choosing to define a PDE using PIDEProblem, note that the function being integrated f is a function of f(x, y, v_x, v_y, ∇v_x, ∇v_y) out of which y is the integrating variable and x is constant throughout the integration. If a PDE has no integral and the non linear term f is just evaluated as f(x, v_x, ∇v_x) then we suggest using ParabolicPDEProblem","category":"page"},{"location":"tutorials/deepbsde/#Solving-a-100-dimensional-Hamilton-Jacobi-Bellman-Equation-with-DeepBSDE","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"First, here's a fully working code for the solution of a 100-dimensional Hamilton-Jacobi-Bellman equation that takes a few minutes on a laptop:","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"using HighDimPDE\nusing Flux\nusing StochasticDiffEq\nusing LinearAlgebra\n\nd = 100 # number of dimensions\nx0 = fill(0.0f0, d)\ntspan = (0.0f0, 1.0f0)\ndt = 0.2f0\nλ = 1.0f0\n#\ng(X) = log(0.5f0 + 0.5f0 * sum(X .^ 2))\nf(X, u, σᵀ∇u, p, t) = -λ * sum(σᵀ∇u .^ 2)\nμ_f(X, p, t) = zero(X)  #Vector d x 1 λ\nσ_f(X, p, t) = Diagonal(sqrt(2.0f0) * ones(Float32, d)) #Matrix d x d\nprob = ParabolicPDEProblem(μ_f, σ_f, x0, tspan; g, f)\n\nhls = 10 + d #hidden layer size\nopt = Flux.Optimise.Adam(0.1)  #optimizer\n#sub-neural network approximating solutions at the desired point\nu0 = Flux.Chain(Dense(d, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, 1))\n# sub-neural network approximating the spatial gradients at time point\nσᵀ∇u = Flux.Chain(Dense(d + 1, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, d))\npdealg = DeepBSDE(u0, σᵀ∇u, opt = opt)\n\n@time sol = solve(prob,\n    pdealg,\n    StochasticDiffEq.EM(),\n    verbose = true,\n    maxiters = 150,\n    trajectories = 30,\n    dt = 1.2f0,\n    pabstol = 1.0f-4)","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"Now, let's explain the details!","category":"page"},{"location":"tutorials/deepbsde/#Hamilton-Jacobi-Bellman-Equation","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Hamilton-Jacobi-Bellman Equation","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"The Hamilton-Jacobi-Bellman equation is the solution to a stochastic optimal control problem.","category":"page"},{"location":"tutorials/deepbsde/#Symbolic-Solution","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Symbolic Solution","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"Here, we choose to solve the classical Linear Quadratic Gaussian (LQG) control problem of 100 dimensions, which is governed by the SDE","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"d X_t = 2 sqrtlambda c_t dt + sqrt2dW_t","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"where c_t is a control process. The solution to the optimal control is given by a PDE of the form:","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"partial_t u(tx) + Delta_x u + lambda  Nabla u (tx)^2 = 0 ","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"with terminating condition g(x) = log(12 + 12 x^2)).","category":"page"},{"location":"tutorials/deepbsde/#Solving-LQG-Problem-with-Neural-Net","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving LQG Problem with Neural Net","text":"","category":"section"},{"location":"tutorials/deepbsde/#Define-the-Problem","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Define the Problem","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"To get the solution above using the ParabolicPDEProblem, we write:","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"using HighDimPDE\nusing Flux\nusing StochasticDiffEq\nusing LinearAlgebra\n\nd = 100 # number of dimensions\nX0 = fill(0.0f0, d) # initial value of stochastic control process\ntspan = (0.0f0, 1.0f0)\nλ = 1.0f0\n\ng(X) = log(0.5f0 + 0.5f0 * sum(X .^ 2))\nf(X, u, σᵀ∇u, p, t) = -λ * sum(σᵀ∇u .^ 2)\nμ_f(X, p, t) = zero(X)  #Vector d x 1 λ\nσ_f(X, p, t) = Diagonal(sqrt(2.0f0) * ones(Float32, d)) #Matrix d x d\nprob = ParabolicPDEProblem(μ_f, σ_f, X0, tspan; g, f)","category":"page"},{"location":"tutorials/deepbsde/#Define-the-Solver-Algorithm","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Define the Solver Algorithm","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"As described in the API docs, we now need to define our DeepBSDE algorithm by giving it the Flux.jl chains we want it to use for the neural networks. u0 needs to be a d dimensional -> 1 dimensional chain, while σᵀ∇u needs to be d+1 dimensional to d dimensions. Thus we define the following:","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"hls = 10 + d #hidden layer size\nopt = Flux.Optimise.Adam(0.01)  #optimizer\n#sub-neural network approximating solutions at the desired point\nu0 = Flux.Chain(Dense(d, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, 1))\n# sub-neural network approximating the spatial gradients at time point\nσᵀ∇u = Flux.Chain(Dense(d + 1, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, d))\npdealg = DeepBSDE(u0, σᵀ∇u, opt = opt)","category":"page"},{"location":"tutorials/deepbsde/#Solving-with-Neural-Nets","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving with Neural Nets","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"@time ans = solve(prob, pdealg, EM(), verbose = true, maxiters = 100,\n    trajectories = 100, dt = 0.2f0, pabstol = 1.0f-2)","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"Here we want to solve the underlying neural SDE using the Euler-Maruyama SDE solver with our chosen dt=0.2, do at most 100 iterations of the optimizer, 100 SDE solves per loss evaluation (for averaging), and stop if the loss ever goes below 1f-2.","category":"page"},{"location":"tutorials/deepbsde/#Solving-the-100-dimensional-Black-Scholes-Barenblatt-Equation","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving the 100-dimensional Black-Scholes-Barenblatt Equation","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"Black Scholes equation is a model for stock option price. In 1973, Black and Scholes transformed their formula on option pricing and corporate liabilities into a PDE model, which is widely used in financing engineering for computing the option price over time. [1.] In this example, we will solve a Black-Scholes-Barenblatt equation of 100 dimensions. The Black-Scholes-Barenblatt equation is a nonlinear extension to the Black-Scholes equation, which models uncertain volatility and interest rates derived from the Black-Scholes equation. This model results in a nonlinear PDE whose dimension is the number of assets in the portfolio.","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"To solve it using the ParabolicPDEProblem, we write:","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"d = 100 # number of dimensions\nX0 = repeat([1.0f0, 0.5f0], div(d, 2)) # initial value of stochastic state\ntspan = (0.0f0, 1.0f0)\nr = 0.05f0\nsigma = 0.4f0\nf(X, u, σᵀ∇u, p, t) = r * (u - sum(X .* σᵀ∇u))\ng(X) = sum(X .^ 2)\nμ_f(X, p, t) = zero(X) #Vector d x 1\nσ_f(X, p, t) = Diagonal(sigma * X) #Matrix d x d\nprob = ParabolicPDEProblem(μ_f, σ_f, X0, tspan; g, f)","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"As described in the API docs, we now need to define our NNPDENS algorithm by giving it the Flux.jl chains we want it to use for the neural networks. u0 needs to be a d-dimensional -> 1-dimensional chain, while σᵀ∇u needs to be d+1-dimensional to d dimensions. Thus we define the following:","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"hls = 10 + d #hide layer size\nopt = Flux.Optimise.Adam(0.001)\nu0 = Flux.Chain(Dense(d, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, 1))\nσᵀ∇u = Flux.Chain(Dense(d + 1, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, d))\npdealg = DeepBSDE(u0, σᵀ∇u, opt = opt)","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"And now we solve the PDE. Here, we say we want to solve the underlying neural SDE using the Euler-Maruyama SDE solver with our chosen dt=0.2, do at most 150 iterations of the optimizer, 100 SDE solves per loss evaluation (for averaging), and stop if the loss ever goes below 1f-6.","category":"page"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"ans = solve(\n    prob, pdealg, EM(), verbose = true, maxiters = 150, trajectories = 100, dt = 0.2f0)","category":"page"},{"location":"tutorials/deepbsde/#References","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"References","text":"","category":"section"},{"location":"tutorials/deepbsde/","page":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","title":"Solving a 100-dimensional Hamilton-Jacobi-Bellman Equation with DeepBSDE","text":"Shinde, A. S., and K. C. Takale. \"Study of Black-Scholes model and its applications.\" Procedia Engineering 38 (2012): 270-279.","category":"page"},{"location":"NNKolmogorov/#nn_kolmogorov","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"","category":"section"},{"location":"NNKolmogorov/#Problems-Supported:","page":"The NNKolmogorov algorithm","title":"Problems Supported:","text":"","category":"section"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"ParabolicPDEProblem","category":"page"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"Modules = [HighDimPDE]\nPages   = [\"NNKolmogorov.jl\"]","category":"page"},{"location":"NNKolmogorov/#HighDimPDE.NNKolmogorov","page":"The NNKolmogorov algorithm","title":"HighDimPDE.NNKolmogorov","text":"Algorithm for solving Kolmogorov Equations.\n\nHighDimPDE.NNKolmogorov(chain, opt)\n\nArguments:\n\nchain: A Chain neural network with a d-dimensional output.\nopt: The optimizer to train the neural network. Defaults to ADAM(0.1).\n\n[1]Beck, Christian, et al. \"Solving stochastic differential equations and Kolmogorov equations by means of deep learning.\" arXiv preprint arXiv:1806.00421 (2018).\n\n\n\n\n\n","category":"type"},{"location":"NNKolmogorov/#CommonSolve.solve-Tuple{ParabolicPDEProblem, NNKolmogorov, Any}","page":"The NNKolmogorov algorithm","title":"CommonSolve.solve","text":"solve(\n    prob::ParabolicPDEProblem,\n    pdealg::NNKolmogorov,\n    sdealg;\n    ensemblealg,\n    abstol,\n    verbose,\n    maxiters,\n    trajectories,\n    save_everystep,\n    use_gpu,\n    dt,\n    dx,\n    kwargs...\n)\n\n\nReturns a PIDESolution object.\n\nArguments\n\nsdealg: a SDE solver from DifferentialEquations.jl.    If not provided, the plain vanilla DeepBSDE method will be applied.   If provided, the SDE associated with the PDE problem will be solved relying on    methods from DifferentialEquations.jl, using Ensemble solves    via sdealg. Check the available sdealg on the    DifferentialEquations.jl doc.\nmaxiters: The number of training epochs. Defaults to 300\ntrajectories: The number of trajectories simulated for training. Defaults to 100\nExtra keyword arguments passed to solve will be further passed to the SDE solver.\n\n\n\n\n\n","category":"method"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"NNKolmogorov obtains a","category":"page"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"terminal solution for Forward Kolmogorov Equations of the form:","category":"page"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"partial_t u(tx) = mu(t x) nabla_x u(tx) + frac12 sigma^2(t x) Delta_x u(tx)","category":"page"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"with initial condition given by g(x)","category":"page"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"or an initial condition for Backward Kolmogorov Equations of the form:","category":"page"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"partial_t u(tx) = - mu(t x) nabla_x u(tx) - frac12 sigma^2(t x) Delta_x u(tx)","category":"page"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"with terminal condition given by g(x)","category":"page"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"We can use the Feynman-Kac formula :","category":"page"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"S_t^x = int_0^tmu(S_s^x)ds + int_0^tsigma(S_s^x)dB_s","category":"page"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"And the solution is given by:","category":"page"},{"location":"NNKolmogorov/","page":"The NNKolmogorov algorithm","title":"The NNKolmogorov algorithm","text":"f(T x) = mathbbEg(S_T^x)","category":"page"},{"location":"#HighDimPDE.jl","page":"Home","title":"HighDimPDE.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"HighDimPDE.jl is a Julia package to solve Highly Dimensional non-linear, non-local PDEs of the forms:","category":"page"},{"location":"#1.-Partial-Integro-Differential-Equations:","page":"Home","title":"1. Partial Integro Differential Equations:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"beginaligned\n    (partial_t u)(tx) = int_Omega fbig(txbf x u(tx)u(tbf x) ( nabla_x u )(tx )( nabla_x u )(tbf x ) big)  dbf x \n     quad + biglangle mu(tx) ( nabla_x u )( tx ) bigrangle + tfrac12 textTrace big(sigma(tx)  sigma(tx) ^* ( textHess_x u)(t x ) big)\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"where u colon 0T times Omega to R, Omega subseteq R^d is subject to initial and boundary conditions, and where d is large. These equations are defined using the PIDEProblem","category":"page"},{"location":"#2.-Parabolic-Partial-Differential-Equations:","page":"Home","title":"2. Parabolic Partial Differential Equations:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"beginaligned\n    (partial_t u)(tx) =  fbig(tx u(tx) ( nabla_x u )(tx )big) \n    + biglangle mu(tx) ( nabla_x u )( tx ) bigrangle + tfrac12 textTrace big(sigma(tx)  sigma(tx) ^* ( textHess_x u)(t x ) big)\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"where u colon 0T times Omega to R, Omega subseteq R^d is subject to initial and boundary conditions, and where d is large. These equations are defined using the ParabolicPDEProblem","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nThe difference between the two problems is that in Partial Integro Differential Equations, the integrand is integrated over x, while in Parabolic Integro Differential Equations, the function f is just evaluated for x.","category":"page"},{"location":"","page":"Home","title":"Home","text":"HighDimPDE.jl implements solver algorithms that break down the curse of dimensionality, including","category":"page"},{"location":"","page":"Home","title":"Home","text":"the Deep Splitting scheme\nthe Multi-Level Picard iterations scheme\nthe Deep BSDE scheme.\nNNKolmogorov and NNParamKolmogorov schemes for Kolmogorov PDEs.\n[NNStopping] scheme for solving optimal stopping time problem.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To make the most out of HighDimPDE.jl, we advise to first have a look at the","category":"page"},{"location":"","page":"Home","title":"Home","text":"documentation on the Feynman Kac formula,","category":"page"},{"location":"","page":"Home","title":"Home","text":"as all solver algorithms heavily rely on it.","category":"page"},{"location":"#Algorithm-overview","page":"Home","title":"Algorithm overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Features DeepSplitting MLP DeepBSDE\nTime discretization free ❌ ✅ ❌\nMesh-free ✅ ✅ ✅\nSingle point x in R^d approximation ✅ ✅ ✅\nd-dimensional cube ab^d approximation ✅ ❌ ✔️\nGPU ✅ ❌ ✅\nGradient non-linearities ✔️ ❌ ✅","category":"page"},{"location":"","page":"Home","title":"Home","text":"✔️ : will be supported in the future","category":"page"},{"location":"#Reproducibility","page":"Home","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"</details>","category":"page"},{"location":"","page":"Home","title":"Home","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"tutorials/deepsplitting/#Solving-the-10-dimensional-Fisher-KPP-equation-with-DeepSplitting","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"","category":"section"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"Consider the Fisher-KPP equation with non-local competition","category":"page"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"partial_t u = u (1 - int_Omega u(ty)dy) + frac12sigma^2Delta_xu tag1","category":"page"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"where Omega = -12 12^d, and let's assume Neumann Boundary condition on Omega.","category":"page"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"Let's solve Eq. (1) with the DeepSplitting solver.","category":"page"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"using HighDimPDE, Flux\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0, 0.5) # time horizon\nx0 = fill(0.0f0, d)  # initial point\ng(x) = exp.(-sum(x .^ 2, dims = 1)) # initial condition\nμ(x, p, t) = 0.0f0 # advection coefficients\nσ(x, p, t) = 0.1f0 # diffusion coefficients\nx0_sample = UniformSampling(fill(-5.0f-1, d), fill(5.0f-1, d))\nf(x, y, v_x, v_y, ∇v_x, ∇v_y, p, t) = v_x .* (1.0f0 .- v_y)","category":"page"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"Since this is a non-local equation, we will define our problem as a PIDEProblem","category":"page"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"prob = PIDEProblem(μ, σ, x0, tspan, g, f; x0_sample = x0_sample)","category":"page"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"## Definition of the neural network to use\n\nhls = d + 50 #hidden layer size\n\nnn = Flux.Chain(Dense(d, hls, tanh),\n    Dense(hls, hls, tanh),\n    Dense(hls, 1)) # neural network used by the scheme\n\nopt = ADAM(1e-2)\n\n## Definition of the algorithm\nalg = DeepSplitting(nn,\n    opt = opt,\n    mc_sample = x0_sample)\n\nsol = solve(prob,\n    alg,\n    0.1,\n    verbose = true,\n    abstol = 2e-3,\n    maxiters = 1000,\n    batch_size = 1000)","category":"page"},{"location":"tutorials/deepsplitting/#Solving-on-the-GPU","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving on the GPU","text":"","category":"section"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"DeepSplitting can run on the GPU for (much) improved performance. To do so, just set use_cuda = true.","category":"page"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"sol = solve(prob,\n    alg,\n    0.1,\n    verbose = true,\n    abstol = 2e-3,\n    maxiters = 1000,\n    batch_size = 1000,\n    use_cuda = true)","category":"page"},{"location":"tutorials/deepsplitting/#Solving-local-Allen-Cahn-Equation-with-Neumann-BC","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving local Allen Cahn Equation with Neumann BC","text":"","category":"section"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"using HighDimPDE, Flux, StochasticDiffEq\nbatch_size = 1000\ntrain_steps = 1000\n\ntspan = (0.0f0, 5.0f-1)\ndt = 5.0f-2  # time step\n\nμ(x, p, t) = 0.0f0 # advection coefficients\nσ(x, p, t) = 1.0f0 # diffusion coefficients\n\nd = 10\n∂ = fill(5.0f-1, d)\n\nhls = d + 50 #hidden layer size\n\nnn = Flux.Chain(Dense(d, hls, relu),\n    Dense(hls, hls, relu),\n    Dense(hls, 1)) # Neural network used by the scheme\n\nopt = Flux.Optimise.Adam(1e-2) #optimiser\nalg = DeepSplitting(nn, opt = opt)\n\nX0 = fill(0.0f0, d)  # initial point\ng(X) = exp.(-0.25f0 * sum(X .^ 2, dims = 1))   # initial condition\na(u) = u - u^3\nf(y, v_y, ∇v_y, p, t) = a.(v_y) # nonlocal nonlinear function","category":"page"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"Since we are dealing with a local problem here, i.e. no integration term used, we use ParabolicPDEProblem to define the problem.","category":"page"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"# defining the problem\nprob = ParabolicPDEProblem(μ, σ, X0, tspan; g, f, neumann_bc = [-∂, ∂])","category":"page"},{"location":"tutorials/deepsplitting/","page":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","title":"Solving the 10-dimensional Fisher-KPP equation with DeepSplitting","text":"# solving\n@time sol = solve(prob,\n    alg,\n    dt,\n    verbose = false,\n    abstol = 1e-5,\n    use_cuda = false,\n    maxiters = train_steps,\n    batch_size = batch_size)","category":"page"}]
}
